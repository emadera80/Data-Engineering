{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 64-bit",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Amazon Data Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed packages\n",
    "import boto3 \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from botocore.exceptions import ClientError\n",
    "import configparser \n",
    "from time import time\n",
    "\n"
   ]
  },
  {
   "source": [
    "Let use the Configpaser to parse the config file and use our variables to create and lauch a cluster on the amazon aws.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                    Param             Value\n0  DWH_CLUSTER_TYPE        multi-node      \n1  DWH_NUM_NODES           4               \n2  DWH_CLUSTER_IDENTIFIER  redshift-cluster\n3  DWH_DB_USER             awsuser         \n4  DWH_DB_PORT             5439            \n5  DWH_IAM_ROLE            myRedshiftRole  \n6  DWH_NODE_TYPE           dc2.large       ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Param</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DWH_CLUSTER_TYPE</td>\n      <td>multi-node</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DWH_NUM_NODES</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DWH_CLUSTER_IDENTIFIER</td>\n      <td>redshift-cluster</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DWH_DB_USER</td>\n      <td>awsuser</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DWH_DB_PORT</td>\n      <td>5439</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>DWH_IAM_ROLE</td>\n      <td>myRedshiftRole</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>DWH_NODE_TYPE</td>\n      <td>dc2.large</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('config.cfg'))\n",
    "\n",
    "KEY = config.get('AWS', 'KEY')\n",
    "SECRET = config.get('AWS', 'SECRET') \n",
    "\n",
    "DWH_CLUSTER_TYPE = config.get('DWH', 'DWH_CLUSTER_TYPE')\n",
    "DWH_NUM_NODES = config.get('DWH', 'DWH_NUM_NODES')\n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH', 'DWH_CLUSTER_IDENTIFIER')\n",
    "DWH_DB = config.get('DWH', 'DWH_DB')\n",
    "DWH_DB_USER = config.get('DWH', 'DWH_DB_USER')\n",
    "DWH_DB_PASSWORD = config.get('DWH', 'DWH_PASSWORD')\n",
    "DWH_DB_PORT = config.get('DWH', 'DWH_PORT')\n",
    "DWH_IAM_ROLE = config.get('DWH', 'DWH_IAM_ROLE')\n",
    "DWH_NODE_TYPE = config.get('DWH', 'DWH_NODE_TYPE')\n",
    "\n",
    "pd.DataFrame ( {'Param': [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB_USER\",  \"DWH_DB_PORT\", \"DWH_IAM_ROLE\", \"DWH_NODE_TYPE\"], \n",
    "                'Value': [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_CLUSTER_IDENTIFIER, DWH_DB_USER, DWH_DB_PORT, DWH_IAM_ROLE, DWH_NODE_TYPE]} )"
   ]
  },
  {
   "source": [
    "Creating the Amazon Instances to Connect to. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2', \n",
    "        region_name = 'us-west-2', \n",
    "        aws_access_key_id = KEY, \n",
    "        aws_secret_access_key = SECRET\n",
    ")\n",
    "\n",
    "s3 = boto3.resource('s3', \n",
    "        region_name = 'us-west-2', \n",
    "        aws_access_key_id = KEY, \n",
    "        aws_secret_access_key = SECRET\n",
    ")\n",
    "\n",
    "iam = boto3.client('iam',\n",
    "        region_name = 'us-west-2', \n",
    "        aws_access_key_id = KEY, \n",
    "        aws_secret_access_key = SECRET\n",
    ")\n",
    "\n",
    "redshift = boto3.client ('redshift',\n",
    "        region_name = 'us-west-2', \n",
    "        aws_access_key_id = KEY, \n",
    "        aws_secret_access_key = SECRET\n",
    "\n",
    ")"
   ]
  },
  {
   "source": [
    "### Testing the S3\n",
    "\n",
    "Since I am using the sample data provided by amazon I am going to test the connection to the sample buckets in amazon.  Below we see we get results back from amazon S3.   \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/customer0002_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/dwdate.tbl.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/lineorder0000_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/lineorder0001_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/lineorder0002_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/lineorder0003_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/lineorder0004_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/lineorder0005_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/lineorder0006_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/lineorder0007_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/part0000_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/part0001_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/part0002_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/part0003_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/supplier.tbl_0000_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/supplier0001_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/supplier0002_part_00.gz&#39;)\ns3.ObjectSummary(bucket_name=&#39;awssampledbuswest2&#39;, key=&#39;ssbgz/supplier0003_part_00.gz&#39;)\n"
    }
   ],
   "source": [
    "sampleBucket = s3.Bucket(\"awssampledbuswest2\")\n",
    "\n",
    "for obj in sampleBucket.objects.filter(Prefix='ssbgz'): \n",
    "    print(obj)"
   ]
  },
  {
   "source": [
    "Before Launch the cluster I am going to create an I AM ROLE. The reason I am doing this is to show case and end to end.  Once a cluster is created with the nessary roles and policies in place all we need to connect to the cluster is endpoint, and the credentials.    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Created a new I AM Role\nname &#39;DWH_IAM_ROLENAME&#39; is not defined\nAttached Policy\nGet I am role ARN\n"
    }
   ],
   "source": [
    "try: \n",
    "    print('Created a new I AM Role')\n",
    "    dwhrole = iam.create_role(\n",
    "        Path='/', \n",
    "        RoleName=DWH_IAM_ROLE, \n",
    "        Description='Allows Redshift clusters to call aws services on my behalf',\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {\n",
    "                'Statement': [{\n",
    "                    'Actiion': 'sts:AssumeRole', \n",
    "                    'Effect': 'Allow', \n",
    "                    'Principal': {\n",
    "                        'Service':'redshift-amazonaws.com'\n",
    "                    }\n",
    "                }], \n",
    "                'Version': '2012-10-17'\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "\n",
    "print('Attached Policy')\n",
    "\n",
    "iam.attach_role_policy(\n",
    "    RoleName=DWH_IAM_ROLE, \n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
    ")['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "\n",
    "ARNROLE = iam.get_role(RoleName=DWH_IAM_ROLE)['Role']['Arn']\n"
   ]
  },
  {
   "source": [
    "### Create and Launch Redshift Cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    response = redshift.create_cluster(\n",
    "        ClusterType = DWH_CLUSTER_TYPE, \n",
    "        NodeType = DWH_NODE_TYPE, \n",
    "        NumberOfNodes = int(DWH_NUM_NODES), \n",
    "        DBName = DWH_DB, \n",
    "        ClusterIdentifier = DWH_CLUSTER_IDENTIFIER, \n",
    "        MasterUsername = DWH_DB_USER, \n",
    "        MasterUserPassword = DWH_DB_PASSWORD,\n",
    "\n",
    "        IamRoles = [ARNROLE]\n",
    "    )\n",
    "except Exception as e: \n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "Use the below code to see the status of the cluster, you may need to execute this code a number of time until you see cluster becoming \"Available\".  May take serveral minutes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 Key             Value\n0  ClusterIdentifier  redshift-cluster\n1  NodeType           dc2.large       \n2  ClusterStatus      available       \n3  MasterUsername     awsuser         \n4  DBName             dev             \n5  NumberOfNodes      4               ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Key</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ClusterIdentifier</td>\n      <td>redshift-cluster</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NodeType</td>\n      <td>dc2.large</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ClusterStatus</td>\n      <td>available</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MasterUsername</td>\n      <td>awsuser</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DBName</td>\n      <td>dev</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NumberOfNodes</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"NumberOfNodes\"]\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "source": [
    "After the cluster becomes available we need to access the cluster endpoint and ARN role we will do this by using the below code. We can print the results of this and lets make a note of the endpoint and role arn we going to need that info to connect to the cluster. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']"
   ]
  },
  {
   "source": [
    "Before we proceed to connecting to the cluster we need to open port to be able to connect from the outside.   I already have define the port so this already exists.  If you haven't you will see a different message.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ec2.SecurityGroup(id=&#39;sg-8c8917a6&#39;)\nAn error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule &quot;peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW&quot; already exists\n"
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_DB_PORT),\n",
    "        ToPort=int(DWH_DB_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "### CONNECTING TO CLUSTER"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_DB_PORT,DWH_DB)\n",
    "%sql $conn_string"
   ]
  },
  {
   "source": [
    "### CREATE THE SCHEMA AND TABLE\n",
    "\n",
    "I have decided to create a distributed table schema this reason for this is using this schema once the tables are loaded we can quickly query our data. \n",
    "\n",
    "What this will do is to create partition on the table. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS dist;\n",
    "SET search_path TO dist;\n",
    "\n",
    "DROP TABLE IF EXISTS part cascade;\n",
    "DROP TABLE IF EXISTS supplier;\n",
    "DROP TABLE IF EXISTS supplier;\n",
    "DROP TABLE IF EXISTS customer;\n",
    "DROP TABLE IF EXISTS dwdate;\n",
    "DROP TABLE IF EXISTS lineorder;\n",
    "\n",
    "CREATE TABLE part (\n",
    "  p_partkey     \tinteger     \tnot null\tsortkey distkey,\n",
    "  p_name        \tvarchar(22) \tnot null,\n",
    "  p_mfgr        \tvarchar(6)      not null,\n",
    "  p_category    \tvarchar(7)      not null,\n",
    "  p_brand1      \tvarchar(9)      not null,\n",
    "  p_color       \tvarchar(11) \tnot null,\n",
    "  p_type        \tvarchar(25) \tnot null,\n",
    "  p_size        \tinteger     \tnot null,\n",
    "  p_container   \tvarchar(10)     not null\n",
    ");\n",
    "\n",
    "CREATE TABLE supplier (\n",
    "  s_suppkey     \tinteger        not null sortkey,\n",
    "  s_name        \tvarchar(25)    not null,\n",
    "  s_address     \tvarchar(25)    not null,\n",
    "  s_city        \tvarchar(10)    not null,\n",
    "  s_nation      \tvarchar(15)    not null,\n",
    "  s_region      \tvarchar(12)    not null,\n",
    "  s_phone       \tvarchar(15)    not null)\n",
    "diststyle all;\n",
    "\n",
    "CREATE TABLE customer (\n",
    "  c_custkey     \tinteger        not null sortkey,\n",
    "  c_name        \tvarchar(25)    not null,\n",
    "  c_address     \tvarchar(25)    not null,\n",
    "  c_city        \tvarchar(10)    not null,\n",
    "  c_nation      \tvarchar(15)    not null,\n",
    "  c_region      \tvarchar(12)    not null,\n",
    "  c_phone       \tvarchar(15)    not null,\n",
    "  c_mktsegment      varchar(10)    not null)\n",
    "diststyle all;\n",
    "\n",
    "CREATE TABLE dwdate (\n",
    "  d_datekey            integer       not null sortkey,\n",
    "  d_date               varchar(19)   not null,\n",
    "  d_dayofweek\t      varchar(10)   not null,\n",
    "  d_month      \t    varchar(10)   not null,\n",
    "  d_year               integer       not null,\n",
    "  d_yearmonthnum       integer  \t not null,\n",
    "  d_yearmonth          varchar(8)\tnot null,\n",
    "  d_daynuminweek       integer       not null,\n",
    "  d_daynuminmonth      integer       not null,\n",
    "  d_daynuminyear       integer       not null,\n",
    "  d_monthnuminyear     integer       not null,\n",
    "  d_weeknuminyear      integer       not null,\n",
    "  d_sellingseason      varchar(13)    not null,\n",
    "  d_lastdayinweekfl    varchar(1)    not null,\n",
    "  d_lastdayinmonthfl   varchar(1)    not null,\n",
    "  d_holidayfl          varchar(1)    not null,\n",
    "  d_weekdayfl          varchar(1)    not null)\n",
    "diststyle all;\n",
    "\n",
    "CREATE TABLE lineorder (\n",
    "  lo_orderkey      \t    integer     \tnot null,\n",
    "  lo_linenumber        \tinteger     \tnot null,\n",
    "  lo_custkey           \tinteger     \tnot null,\n",
    "  lo_partkey           \tinteger     \tnot null distkey,\n",
    "  lo_suppkey           \tinteger     \tnot null,\n",
    "  lo_orderdate         \tinteger     \tnot null sortkey,\n",
    "  lo_orderpriority     \tvarchar(15)     not null,\n",
    "  lo_shippriority      \tvarchar(1)      not null,\n",
    "  lo_quantity          \tinteger     \tnot null,\n",
    "  lo_extendedprice     \tinteger     \tnot null,\n",
    "  lo_ordertotalprice   \tinteger     \tnot null,\n",
    "  lo_discount          \tinteger     \tnot null,\n",
    "  lo_revenue           \tinteger     \tnot null,\n",
    "  lo_supplycost        \tinteger     \tnot null,\n",
    "  lo_tax               \tinteger     \tnot null,\n",
    "  lo_commitdate         integer         not null,\n",
    "  lo_shipmode          \tvarchar(10)     not null\n",
    ");"
   ]
  },
  {
   "source": [
    "### LOADING THE TABLES\n",
    "\n",
    "The below function will load the tables from S3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTables(schema, tables):\n",
    "    loadTimes = []\n",
    "    SQL_SET_SCEMA = \"SET search_path TO {};\".format(schema)\n",
    "    %sql $SQL_SET_SCEMA\n",
    "    \n",
    "    for table in tables:\n",
    "        SQL_COPY = \"\"\"\n",
    "copy {} from 's3://awssampledbuswest2/ssbgz/{}' \n",
    "credentials 'aws_iam_role={}'\n",
    "gzip region 'us-west-2';\n",
    "        \"\"\".format(table,table, ARNROLE)\n",
    "\n",
    "        print(\"======= LOADING TABLE: ** {} ** IN SCHEMA ==> {} =======\".format(table, schema))\n",
    "        print(SQL_COPY)\n",
    "\n",
    "        t0 = time()\n",
    "        %sql $SQL_COPY\n",
    "        loadTime = time()-t0\n",
    "        loadTimes.append(loadTime)\n",
    "\n",
    "        print(\"=== DONE IN: {0:.2f} sec\\n\".format(loadTime))\n",
    "    return pd.DataFrame({\"table\":tables, \"loadtime_\"+schema:loadTimes}).set_index('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    }
   ],
   "source": [
    "# list tables and have the function to take care the loading. \n",
    "\n",
    "tables = [\"customer\",\"dwdate\",\"supplier\", \"part\", \"lineorder\"]\n",
    "\n",
    "#-- Insertion twice for each schema (WARNING!! EACH CAN TAKE MORE THAN 10 MINUTES!!!)\n",
    "distStats = loadTables(\"dist\", tables)"
   ]
  },
  {
   "source": [
    "Below query will be checking the contents of our lineorder table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1 rows affected. 600037902\n"
    }
   ],
   "source": [
    "%%sql \n",
    "SELECT count(*) \n",
    "FROM dist.lineorder; "
   ]
  },
  {
   "source": [
    "### CLEANING UP THE RESOURCES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    }
   ],
   "source": [
    "print('')#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 Key             Value\n0  ClusterIdentifier  redshift-cluster\n1  NodeType           dc2.large       \n2  ClusterStatus      deleting        \n3  MasterUsername     awsuser         \n4  DBName             dev             \n5  NumberOfNodes      4               ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Key</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ClusterIdentifier</td>\n      <td>redshift-cluster</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NodeType</td>\n      <td>dc2.large</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ClusterStatus</td>\n      <td>deleting</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MasterUsername</td>\n      <td>awsuser</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DBName</td>\n      <td>dev</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NumberOfNodes</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{&#39;ResponseMetadata&#39;: {&#39;RequestId&#39;: &#39;08a89ce6-4928-44aa-8408-8a61004e7fa5&#39;,\n  &#39;HTTPStatusCode&#39;: 200,\n  &#39;HTTPHeaders&#39;: {&#39;x-amzn-requestid&#39;: &#39;08a89ce6-4928-44aa-8408-8a61004e7fa5&#39;,\n   &#39;content-type&#39;: &#39;text/xml&#39;,\n   &#39;content-length&#39;: &#39;200&#39;,\n   &#39;date&#39;: &#39;Thu, 01 Oct 2020 23:25:33 GMT&#39;},\n  &#39;RetryAttempts&#39;: 0}}"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "source": [
    "THE END"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}